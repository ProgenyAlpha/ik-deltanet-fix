# Fused DeltaNet Kernel Race Condition — Investigation Report

**Target:** ik_llama.cpp PR #1251 (Qwen3-Coder-Next support, AI-generated by Codex 5.3)
**Repo:** https://github.com/YurkoHoshko/ik_llama.cpp
**Hardware:** Minisforum N5 Pro NAS — AMD Ryzen AI 9 HX PRO 370, 12c/24t, 96GB DDR5, AVX-512
**Model:** Qwen3-Coder-Next Q4_K (80B MoE, 3B active, ~41GB GGUF)
**Infrastructure:** Docker on TrueNAS, model at `/mnt/Aether/appdata/ollama/models/blobs/sha256-30e51a7cb1cf1333b9e298b90b4c7790fe2572d8736b002482a0ac96328a2ffb`

## Executive Summary

Three separate CPU bugs identified in PR #1251's Codex-generated DeltaNet implementation:

1. **Fused generation broken** — The fused kernel produces garbage during multi-threaded generation (T=1). Race condition in `ggml_cont_4d(ggml_permute(...))` ops, NOT the kernel itself. Works with `-t 1`.
2. **Non-FA path broken** — Codex marked CPU FA as "unstable" and disabled it by default. The non-FA code path produces bad PPL. Fix: remove the override, re-enable FA.
3. **Chunked path PPL degraded** — `build_delta_net_chunking` produces significantly worse perplexity than the fused kernel (4.17 vs 3.01 at batch 1). Separate implementation bug.

**Root cause of fused generation bug:** The dispatch at line ~4980 sends ALL tokens through `build_delta_net_fused` when fused mode is on, including T=1 generation tokens. The fused path runs 5 `ggml_cont_4d(ggml_permute(...))` ops that have a multi-threading race condition for small tensor shapes (T=1). The non-fused autoregressive path (`build_delta_net_autoregressive`) handles T=1 correctly with standard GGML ops but is never called when fused is enabled.

## Proven Facts

### 1. Thread count determines correctness
| Config | Capital of France | 2+2 | Haiku | Overall |
|--------|------------------|-----|-------|---------|
| `-t 1` (any build) | Paris ✓ | 4 ✓ | Coherent ✓ | PERFECT |
| `-t 12 --no-fused-delta` | Paris ✓ | 2 (wrong) | Coherent ✓ | WORKS |
| `-t 12` fused (default) | UTF-8 error ✗ | Wrong ✗ | Garbage ✗ | BROKEN |
| `-t 12` fused + single-thread kernel patch (v14) | Garbage ✗ | Garbage ✗ | Garbage ✗ | STILL BROKEN |

### 2. Kernel math is proven correct (v12 deep debug)
- Step-by-step tracing of head 0, token 0 shows exact numerical match
- `out_t[0] = out_val + v_new*attn_score` computes correctly
- Output not corrupted by subsequent state update
- No pointer aliasing between output and state regions (verified at runtime)

### 3. Kernel memory layout has no overlaps between threads
- Thread partitioning: `heads_per_thread = ceil(32/12) = 3`, contiguous head ranges
- Output: each head writes to `out_data[h*128 + t*4096 ... h*128+127 + t*4096]` — non-overlapping
- State: each head owns `state_out[h*16384 ... (h+1)*16384-1]` — non-overlapping
- `v_new_buf`: per-thread malloc — thread-local
- `state_in` and `dst`: NOT aliased (confirmed by allocator code AND runtime pointer check)

### 4. GGML barriers are solid
- `ggml_barrier()` after every op in graph compute thread loop (line 26266)
- OpenMP `#pragma omp barrier` when using OpenMP (line 4504)
- All threads sync before proceeding to next graph node

### 5. The race is OUTSIDE the kernel
- **v14 proof:** Forced `if (params->ith != 0) return;` in the kernel, so only thread 0 runs it (all 32 heads, single-threaded). With `-t 12`, output is still garbage.
- **v14 with `-t 1`:** Works perfectly.
- Therefore, the multi-threading bug is in OTHER ops that run with 12 threads.

## Architecture: Three DeltaNet Paths

### Fused Path (default)
```
Input [S,H,T,B] → permute+cont → [S,T,H,B] → ggml_delta_net kernel → view+cont → output [S,H,T,B]
```
Uses `ggml_cont_4d(ggml_permute(...))` for q, k, v, g, beta (5 permute+cont ops).

### Autoregressive Path (`--no-fused-delta`, T=1)
```
Input [S,H,1,B] → standard GGML ops (mul, transpose, sum_rows, etc.) → output
```
No permute+cont pattern. All well-tested ops.

### Chunked Path (`--no-fused-delta`, T>1)
```
Input → chunk processing via GGML ops → output
```
Also no custom kernel.

## Key Hypothesis: `ggml_cont_4d(ggml_permute(...))` Threading Bug (T=1 specific)

The ONLY difference between the working non-fused path and the broken fused path:
- Fused: 5x `ggml_cont_4d(ggml_permute(...))` ops + 1x ggml_delta_net kernel + 1x output cont
- Non-fused: Standard GGML ops (mul_mat, transpose, sum_rows, etc.)

The `GGML_OP_CONT` op handles copying from non-contiguous (permuted) source to contiguous destination. With 12 threads, each thread copies a portion. If this multi-threaded copy has a bug for specific permutation patterns, it would corrupt the kernel's inputs.

**Refined by ikawrakow's PPL data:** The fused path produces **correct PPL during prefill** (T>1) but **garbage during generation** (T=1). This means the CONT threading bug is **shape-dependent** — it triggers when T=1 (small inner dimension after permute) but not when T is large. With T=1, the work partitioning across 12 threads may create degenerate ranges or off-by-one errors in `ggml_compute_forward_dup_f32`.

**Why this wasn't caught before:** Standard transformer attention uses permute(0,2,1,3) too, but with T=1 the tensor is already effectively contiguous in the permuted dimensions, so the CONT op is a no-op or trivial copy. The delta-net permute pattern applied to 4D tensors with T=1 may exercise a rarely-tested code path.

## Proposed Fix: Hybrid Dispatch

The dispatch at `src/llama-build-context.cpp:~4980` currently sends ALL tokens through fused when enabled:

```cpp
// CURRENT (broken for T=1 generation)
if (use_fused_delta_net) {
    attn_out = build_delta_net_fused(q, k, v, gate, beta, state, il);
} else {
    attn_out = n_tok == 1
        ? build_delta_net_autoregressive(...)
        : build_delta_net_chunking(...);
}
```

Proposed fix — fused for prefill only, autoregressive for generation:

```cpp
// FIXED: fused for T>1 (correct PPL), autoregressive for T=1 (correct output)
if (use_fused_delta_net && n_tok > 1) {
    attn_out = build_delta_net_fused(q, k, v, gate, beta, state, il);
} else if (n_tok == 1) {
    attn_out = build_delta_net_autoregressive(q, k, v, gate, beta, state, il);
} else {
    attn_out = build_delta_net_chunking(q, k, v, gate, beta, state, causal_mask, identity, diag_mask, il);
}
```

### v15 Hybrid Test Results (PARTIAL FIX)

Built and tested as `ik-pr1251-v15-hybrid`. Results with `-t 12`:

| Prompt | v15 hybrid (fused PP + auto TG) | --no-fused-delta (chunked PP + auto TG) |
|--------|--------------------------------|----------------------------------------|
| Capital of France | Coherent (pedantic but correct) | Coherent, correct |
| 7 × 8 | Coherent (wrong math, model quality) | Coherent |
| CPU cache explanation | Correct first ~100 tokens, then degenerates into repetition/HTML hallucination | Coherent full 289-token response |

**Verdict:** Hybrid dispatch eliminates the garbage output (huge improvement over immediate corruption), but the fused kernel's prefill state is not fully compatible with the autoregressive generation path. The two paths compute DeltaNet state differently, causing gradual degradation during generation as accumulated state mismatch grows.

**Recommended path:** Use `--no-fused-delta` (chunked + autoregressive) for correct output, or adopt mainline implementation as ikawrakow suggested. The fused kernel needs its own T=1 threading fix rather than routing T=1 to autoregressive.

## Required Patches (Working Non-Fused Build)

### Patch 1: SSM_DT tensor name compat (Ollama GGUF)
```bash
sed -i 's|tn(LLM_TENSOR_SSM_DT,         "bias",   i), {hparams.ssm_dt_rank}|tn(LLM_TENSOR_SSM_DT,                    i), {hparams.ssm_dt_rank}|' src/llama-load-tensors.cpp
```

### Patch 2: Per-layer n_embd_k/v_gqa
```bash
sed -i '/\/\/ Full-attention layer/a\            const int64_t n_embd_k_gqa = hparams.n_embd_k_gqa(i);\n            const int64_t n_embd_v_gqa = hparams.n_embd_v_gqa(i);' src/llama-load-tensors.cpp
```

### Patch 3: Per-layer n_head_kv
```bash
sed -i '/build_layer_attn = \[&\]/a\        const int64_t n_head_kv = hparams.n_head_kv(il);' src/llama-build-context.cpp
```

### Patch 4: g permutation (verified no-op, kept for consistency)
```bash
sed -i 's|ggml_permute(ctx0, g, 2, 0, 3, 1)|ggml_permute(ctx0, g, 1, 0, 2, 3)|' src/llama-build-context.cpp
```

## Performance Baseline

### Our hardware (AMD Ryzen AI 9 HX PRO 370, 12c/24t, CPU-only, Q4_K)

| Config | Prompt tok/s | Gen tok/s |
|--------|-------------|-----------|
| Ollama (baseline) | — | 7.74 |
| `--no-fused-delta -t 12` | ~63 | ~9.5 |
| `-t 1` fused | ~10 | ~15.6 |

### ikawrakow's hardware (Ryzen 3995WX, CPU-only, IQ4_XS)

| Config | PP-2048 tok/s | TG-128 tok/s | PPL [1] |
|--------|--------------|-------------|---------|
| Upstream llama.cpp | 70.4 | 7.23 | 2.99 |
| PR #1251, fused on, FA on | 110.6 | 11.63 | 3.01 (correct) |
| PR #1251, fused off, FA on | 162 | 7.25 | 4.17 (degraded) |
| PR #1251, fused on, FA off | — | — | 17.5 (broken) |

**Key insight:** Fused kernel produces correct PPL during prefill (T>1) but garbage during generation (T=1). Non-fused PP is actually faster (162 vs 110 t/s) but has degraded PPL from the chunked path bug. Non-fused TG matches upstream (7.25 vs 7.23).

## v16 Mainline Transplant Results

### Approach
Replaced Codex's `build_delta_net_chunking` and `build_delta_net_autoregressive` with upstream llama.cpp's implementations (`src/models/qwen3next.cpp`). Removed the fused kernel dispatch entirely.

### Key adaptation for ik_llama.cpp
- ik's `ggml_sub` uses `ggml_are_same_shape()` (strict, no broadcast)
- ik's `ggml_mul` uses `ggml_can_repeat()` (supports broadcast)
- Upstream code relies on `ggml_sub` broadcasting — had to add explicit `ggml_repeat_4d()` for:
  - `gcs_i` (g_cumsum): `[chunk_size, 1, ...]` → `[chunk_size, chunk_size, ...]`
  - `g_last` in g_diff: `[1, 1, ...]` → `[chunk_size, 1, ...]`

### Test Results (v16, `-t 12`)

| Prompt | Result |
|--------|--------|
| Capital of France | "The capital of France is Paris." — perfect |
| CPU cache explanation | 241 tokens, fully coherent, no degradation |
| Internet history (long) | Coherent ~200 tokens, then "(a (a (a..." repetition |
| Python function | Coherent ~200 tokens, then "1, 1, 1, ..." repetition |

### Comparison: v16 Upstream vs v7 Codex (same `--no-fused-delta` test)

**Same degradation pattern on both.** v7 Codex chunking also produces "a flame, a flame..." repetition on the internet history prompt. Confirms the issue is inherent to the chunked DeltaNet path (Bug #3), NOT specific to our transplant.

### Performance

| Config | PP tok/s | TG tok/s |
|--------|----------|----------|
| v16 mainline, `-t 12` | 58-94 | 9.0-9.7 |
| v7 Codex, `--no-fused-delta -t 12` | 84 | 9.6 |
| Same hardware, Vulkan Q8_0 (llama.cpp #19396) | N/A | ~11 |

### Cross-request state contamination
Server reuses KV cache slot between requests. DeltaNet recurrence state leaks between conversations. After "CPU cache" prompt, next "7×8" prompt hallucinated about cache layers. **Workaround: restart between requests.** This is a server-side issue, not our fix.

## Docker Images on NAS

| Image | Description | Status |
|-------|-------------|--------|
| `ik-pr1251-v7` | Patches 1-4, no kernel mods | Working with `--no-fused-delta` |
| `ik-pr1251-v12-deep` | Deep debug instrumentation | Proved kernel math correct |
| `ik-pr1251-v13-ntasks1` | n_tasks=1 attempt | Failed (nth still 12 in kernel) |
| `ik-pr1251-v14` | Single-thread kernel patch | Proved race is outside kernel |
| `ik-pr1251-v15-hybrid` | Hybrid dispatch (fused PP + auto TG) | Partial fix — no garbage but degrades after ~100 tokens |
| `ik-pr1251-v16-mainline` | Upstream transplant (chunked + auto) | **Working** — same quality as Codex non-fused |

## Next Steps

### Priority 1: Root-cause the CONT threading bug (T=1)
- Read `ggml_compute_forward_dup_f32` — find the f32-from-permuted code path
- Determine why T=1 tensor shapes trigger the race but T>1 doesn't
- Test with `-t 2` to find minimum thread count for failure
- This is the real fix — would make fused work for both prefill AND generation

### Priority 2: Chunked path quality degradation (Bug #3)
- Both Codex and upstream chunking degrade after ~200 tokens on long-form prose
- The fused kernel (PPL 3.01) vs chunked (PPL 4.17) gap suggests numerical differences
- May need fused for both PP and TG — which requires fixing the T=1 threading bug

### Priority 3: Address Codex regressions
- ikawrakow noted Codex reverted his MoE row counting optimization (the major MoE bottleneck fix)
- CUDA PP is 3.3x slower than upstream because of these regressions
- Codex also disabled FA by default for CPU — need to remove that override

### ~~Priority 1 (old): Test hybrid dispatch fix~~ — DONE (v15)
- Hybrid dispatch eliminates garbage but fused/autoregressive state incompatibility causes degradation
- Not a viable production fix on its own

### PR Comments
- Posted findings: https://github.com/ikawrakow/ik_llama.cpp/pull/1251#issuecomment-3879689776
- v15 hybrid results: https://github.com/ikawrakow/ik_llama.cpp/pull/1251#issuecomment-3880057796
- 3-bug summary: https://github.com/ikawrakow/ik_llama.cpp/pull/1251#issuecomment-3880064369
- ikawrakow independently confirmed CPU broken, suggested copying mainline implementation

## Agent Architecture Recommendation

For the next debugging session, use a **two-agent collaborative approach** — pair debugging, not assembly line:

### Agent A: "Analyst" (Opus)
- Reads codebase, forms hypotheses, spots patterns in results
- Proposes next investigative step (read specific code, try specific patch, check specific value)

### Agent B: "Operator" (Bash-capable)
- Reads source, runs builds, executes tests, reports raw results
- But also **pushes back** — "that hypothesis doesn't hold because I see X at line Y"

### Key: Conversational, not handoff-based

The agents discuss at each stage rather than operating in strict plan→execute cycles. Agent B isn't a dumb executor — it reads code too and challenges Agent A's assumptions. Agent A doesn't wait for a full build-test cycle to pivot — if Agent B finds contradicting evidence while reading source, they course-correct immediately.

```
A: "What if CONT miscalculates row stride for permute(0,2,1,3) when ne[1] != ne[2]?"
B: "Reading ggml_compute_forward_dup_f32... no, the stride calc looks correct for that case.
    But look — nb[2] here assumes contiguous src, and the permuted tensor isn't contiguous."
A: "That's it. The nb values after permute are reordered but dup_f32 indexes with original
    dimension order. Check if this specific permute pattern [0,2,1,3] hits a different code
    branch than standard attention permutes."
B: [reads code, confirms] "Yes — standard attention uses permute(0,2,1,3) too but the tensor
    shapes are different. For delta_net, ne[1]=T=128 and ne[2]=H=32, so the work partitioning
    across 12 threads splits differently."
```

This prevents:
1. Single-agent context exhaustion (analysis + execution compete for window space)
2. Rigid plan→test loops that waste build cycles on already-invalidated hypotheses
3. One agent missing something the other would catch in real-time

## Key Source Locations

| File | Line | What |
|------|------|------|
| `ggml/src/ggml.c` | 21939 | `ggml_compute_forward_delta_net_f32` — the kernel |
| `ggml/src/ggml.c` | 10235 | `ggml_delta_net()` — op creation, tensor shapes |
| `ggml/src/ggml.c` | 25957 | n_tasks = n_threads for DELTA_NET |
| `ggml/src/ggml.c` | 26239 | Graph compute thread loop with barrier |
| `ggml/src/ggml.c` | 4499 | `ggml_barrier()` implementation |
| `ggml/src/ggml-alloc.c` | 40 | `ggml_op_can_inplace()` — DELTA_NET not listed |
| `src/llama-build-context.cpp` | 4616 | `build_delta_net_fused` — permute+cont ops |
| `src/llama-build-context.cpp` | 4551 | `build_delta_net_autoregressive` — working path |
| `src/llama-build-context.cpp` | 4343 | `build_delta_net_chunking` — prompt path |
| `src/llama-build-context.cpp` | 4980 | Dispatch: fused vs autoregressive vs chunked |
