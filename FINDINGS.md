# Fused DeltaNet Kernel Race Condition — Investigation Report

**Target:** ik_llama.cpp PR #1251 (Qwen3-Coder-Next support, AI-generated by Codex 5.3)
**Repo:** https://github.com/YurkoHoshko/ik_llama.cpp
**Hardware:** Minisforum N5 Pro NAS — AMD Ryzen AI 9 HX PRO 370, 12c/24t, 96GB DDR5, AVX-512
**Model:** Qwen3-Coder-Next Q4_K (80B MoE, 3B active, ~41GB GGUF)
**Infrastructure:** Docker on TrueNAS, model at `/mnt/Aether/appdata/ollama/models/blobs/sha256-30e51a7cb1cf1333b9e298b90b4c7790fe2572d8736b002482a0ac96328a2ffb`

## Executive Summary

The fused DeltaNet CPU kernel (`ggml_compute_forward_delta_net_f32` in `ggml/src/ggml.c:21939`) produces **garbage output with multi-threading** but works perfectly with a single thread. The race condition is **NOT inside the kernel itself** — it's in the surrounding GGML graph ops, likely the `ggml_cont_4d(ggml_permute(...))` data layout transformation ops that prepare tensors for the kernel.

## Proven Facts

### 1. Thread count determines correctness
| Config | Capital of France | 2+2 | Haiku | Overall |
|--------|------------------|-----|-------|---------|
| `-t 1` (any build) | Paris ✓ | 4 ✓ | Coherent ✓ | PERFECT |
| `-t 12 --no-fused-delta` | Paris ✓ | 2 (wrong) | Coherent ✓ | WORKS |
| `-t 12` fused (default) | UTF-8 error ✗ | Wrong ✗ | Garbage ✗ | BROKEN |
| `-t 12` fused + single-thread kernel patch (v14) | Garbage ✗ | Garbage ✗ | Garbage ✗ | STILL BROKEN |

### 2. Kernel math is proven correct (v12 deep debug)
- Step-by-step tracing of head 0, token 0 shows exact numerical match
- `out_t[0] = out_val + v_new*attn_score` computes correctly
- Output not corrupted by subsequent state update
- No pointer aliasing between output and state regions (verified at runtime)

### 3. Kernel memory layout has no overlaps between threads
- Thread partitioning: `heads_per_thread = ceil(32/12) = 3`, contiguous head ranges
- Output: each head writes to `out_data[h*128 + t*4096 ... h*128+127 + t*4096]` — non-overlapping
- State: each head owns `state_out[h*16384 ... (h+1)*16384-1]` — non-overlapping
- `v_new_buf`: per-thread malloc — thread-local
- `state_in` and `dst`: NOT aliased (confirmed by allocator code AND runtime pointer check)

### 4. GGML barriers are solid
- `ggml_barrier()` after every op in graph compute thread loop (line 26266)
- OpenMP `#pragma omp barrier` when using OpenMP (line 4504)
- All threads sync before proceeding to next graph node

### 5. The race is OUTSIDE the kernel
- **v14 proof:** Forced `if (params->ith != 0) return;` in the kernel, so only thread 0 runs it (all 32 heads, single-threaded). With `-t 12`, output is still garbage.
- **v14 with `-t 1`:** Works perfectly.
- Therefore, the multi-threading bug is in OTHER ops that run with 12 threads.

## Architecture: Three DeltaNet Paths

### Fused Path (default)
```
Input [S,H,T,B] → permute+cont → [S,T,H,B] → ggml_delta_net kernel → view+cont → output [S,H,T,B]
```
Uses `ggml_cont_4d(ggml_permute(...))` for q, k, v, g, beta (5 permute+cont ops).

### Autoregressive Path (`--no-fused-delta`, T=1)
```
Input [S,H,1,B] → standard GGML ops (mul, transpose, sum_rows, etc.) → output
```
No permute+cont pattern. All well-tested ops.

### Chunked Path (`--no-fused-delta`, T>1)
```
Input → chunk processing via GGML ops → output
```
Also no custom kernel.

## Key Hypothesis: `ggml_cont_4d(ggml_permute(...))` Threading Bug

The ONLY difference between the working non-fused path and the broken fused path:
- Fused: 5x `ggml_cont_4d(ggml_permute(...))` ops + 1x ggml_delta_net kernel + 1x output cont
- Non-fused: Standard GGML ops (mul_mat, transpose, sum_rows, etc.)

The `GGML_OP_CONT` op handles copying from non-contiguous (permuted) source to contiguous destination. With 12 threads, each thread copies a portion. If this multi-threaded copy has a bug for specific permutation patterns, it would corrupt the kernel's inputs.

**Why this wasn't caught before:** The specific permute patterns used here ([S,H,T,B]→[S,T,H,B]) may be unusual. Standard transformer attention uses different permute patterns that exercise different code paths in `ggml_compute_forward_dup_f32`.

## Required Patches (Working Non-Fused Build)

### Patch 1: SSM_DT tensor name compat (Ollama GGUF)
```bash
sed -i 's|tn(LLM_TENSOR_SSM_DT,         "bias",   i), {hparams.ssm_dt_rank}|tn(LLM_TENSOR_SSM_DT,                    i), {hparams.ssm_dt_rank}|' src/llama-load-tensors.cpp
```

### Patch 2: Per-layer n_embd_k/v_gqa
```bash
sed -i '/\/\/ Full-attention layer/a\            const int64_t n_embd_k_gqa = hparams.n_embd_k_gqa(i);\n            const int64_t n_embd_v_gqa = hparams.n_embd_v_gqa(i);' src/llama-load-tensors.cpp
```

### Patch 3: Per-layer n_head_kv
```bash
sed -i '/build_layer_attn = \[&\]/a\        const int64_t n_head_kv = hparams.n_head_kv(il);' src/llama-build-context.cpp
```

### Patch 4: g permutation (verified no-op, kept for consistency)
```bash
sed -i 's|ggml_permute(ctx0, g, 2, 0, 3, 1)|ggml_permute(ctx0, g, 1, 0, 2, 3)|' src/llama-build-context.cpp
```

## Performance Baseline

| Config | Prompt tok/s | Gen tok/s |
|--------|-------------|-----------|
| Ollama (baseline) | — | 7.74 |
| `--no-fused-delta -t 12` | ~63 | ~9.5 |
| `-t 1` fused | ~10 | ~15.6 |

Fused single-thread has 1.6x better generation but 6x worse prompt processing vs non-fused multi-threaded.

## Docker Images on NAS

| Image | Description | Status |
|-------|-------------|--------|
| `ik-pr1251-v7` | Patches 1-4, no kernel mods | Working with `--no-fused-delta` |
| `ik-pr1251-v12-deep` | Deep debug instrumentation | Proved kernel math correct |
| `ik-pr1251-v13-ntasks1` | n_tasks=1 attempt | Failed (nth still 12 in kernel) |
| `ik-pr1251-v14` | Single-thread kernel patch | Proved race is outside kernel |

## Next Steps

### Priority 1: Isolate the CONT/permute bug
- Test v14 with `-t 2` — does even 2 threads trigger the bug?
- Add checksum verification: compute hash of delta_net input tensors (after cont+permute) and compare single-thread vs multi-thread
- Read `ggml_compute_forward_dup_f32` for the f32-from-non-contiguous code path

### Priority 2: Build a diagnostic version
- Patch the fused path to use `n_tasks=1` for the CONT ops (not the delta_net kernel)
- Or: patch `ggml_compute_forward_dup_f32` to be single-threaded for specific tensor shapes

### Priority 3: Fix or workaround
- If CONT bug confirmed: fix the multi-threading in `ggml_compute_forward_dup_f32` for permuted sources
- Alternative workaround: modify kernel to accept [S,H,T,B] layout directly (skip the cont+permute entirely)
- Simplest workaround: use `--no-fused-delta` flag (already works)

### PR Comment
- Nexesenex commented about `--fused-delta` / `--no-fused-delta` params being redundant
- Our findings are HIGHLY relevant: the fused path has a multi-threading bug
- Should comment on PR with findings once root cause is confirmed

## Agent Architecture Recommendation

For the next debugging session, use a **two-agent approach**:

### Agent A: "Hypothesis Generator" (Opus/Sonnet)
- Reads the codebase, generates specific hypotheses about what's causing the race
- Formulates concrete, testable experiments (specific patches, specific test commands)
- Does NOT execute — only plans

### Agent B: "Test Executor" (Bash-focused)
- Takes Agent A's hypothesis and test plan
- Builds Docker images, runs tests, collects results
- Reports raw results back to Agent A

### Workflow
```
Agent A: "Hypothesis: the CONT op for permute(0,2,1,3) miscalculates thread work ranges
          when ne[1] != ne[2]. Test: patch CONT to single-thread for this case."
   → Agent B builds, tests, reports: "Still broken"
Agent A: "New hypothesis: the IK fusion of SUM_ROWS+DIV at line 23803 matches a node
          adjacent to the CONT output. Test: disable IK fusion and retest."
   → Agent B builds, tests, reports: "Fixed!"
Agent A: "Root cause confirmed. Minimal fix: add guard to fusion pattern."
```

This prevents the single-agent loop problem where analysis and execution compete for context window space.

## Key Source Locations

| File | Line | What |
|------|------|------|
| `ggml/src/ggml.c` | 21939 | `ggml_compute_forward_delta_net_f32` — the kernel |
| `ggml/src/ggml.c` | 10235 | `ggml_delta_net()` — op creation, tensor shapes |
| `ggml/src/ggml.c` | 25957 | n_tasks = n_threads for DELTA_NET |
| `ggml/src/ggml.c` | 26239 | Graph compute thread loop with barrier |
| `ggml/src/ggml.c` | 4499 | `ggml_barrier()` implementation |
| `ggml/src/ggml-alloc.c` | 40 | `ggml_op_can_inplace()` — DELTA_NET not listed |
| `src/llama-build-context.cpp` | 4616 | `build_delta_net_fused` — permute+cont ops |
| `src/llama-build-context.cpp` | 4551 | `build_delta_net_autoregressive` — working path |
| `src/llama-build-context.cpp` | 4343 | `build_delta_net_chunking` — prompt path |
| `src/llama-build-context.cpp` | 4980 | Dispatch: fused vs autoregressive vs chunked |
