# Fused DeltaNet Kernel Race Condition — Investigation Report

**Target:** ik_llama.cpp PR #1251 (Qwen3-Coder-Next support, AI-generated by Codex 5.3)
**Repo:** https://github.com/YurkoHoshko/ik_llama.cpp
**Hardware:** Minisforum N5 Pro NAS — AMD Ryzen AI 9 HX PRO 370, 12c/24t, 96GB DDR5, AVX-512
**Model:** Qwen3-Coder-Next Q4_K (80B MoE, 3B active, ~41GB GGUF)
**Infrastructure:** Docker on TrueNAS, model at `/mnt/Aether/appdata/ollama/models/blobs/sha256-30e51a7cb1cf1333b9e298b90b4c7790fe2572d8736b002482a0ac96328a2ffb`

## Executive Summary

Three separate CPU bugs identified in PR #1251's Codex-generated DeltaNet implementation:

1. **Fused generation broken** — The fused kernel produces garbage during multi-threaded generation (T=1). Race condition in `ggml_cont_4d(ggml_permute(...))` ops, NOT the kernel itself. Works with `-t 1`.
2. **Non-FA path broken** — Codex marked CPU FA as "unstable" and disabled it by default. The non-FA code path produces bad PPL. Fix: remove the override, re-enable FA.
3. **Chunked path PPL degraded** — `build_delta_net_chunking` produces significantly worse perplexity than the fused kernel (4.17 vs 3.01 at batch 1). Separate implementation bug.

**Root cause of fused generation bug:** The dispatch at line ~4980 sends ALL tokens through `build_delta_net_fused` when fused mode is on, including T=1 generation tokens. The fused path runs 5 `ggml_cont_4d(ggml_permute(...))` ops that have a multi-threading race condition for small tensor shapes (T=1). The non-fused autoregressive path (`build_delta_net_autoregressive`) handles T=1 correctly with standard GGML ops but is never called when fused is enabled.

## Proven Facts

### 1. Thread count determines correctness
| Config | Capital of France | 2+2 | Haiku | Overall |
|--------|------------------|-----|-------|---------|
| `-t 1` (any build) | Paris ✓ | 4 ✓ | Coherent ✓ | PERFECT |
| `-t 12 --no-fused-delta` | Paris ✓ | 2 (wrong) | Coherent ✓ | WORKS |
| `-t 12` fused (default) | UTF-8 error ✗ | Wrong ✗ | Garbage ✗ | BROKEN |
| `-t 12` fused + single-thread kernel patch (v14) | Garbage ✗ | Garbage ✗ | Garbage ✗ | STILL BROKEN |

### 2. Kernel math is proven correct (v12 deep debug)
- Step-by-step tracing of head 0, token 0 shows exact numerical match
- `out_t[0] = out_val + v_new*attn_score` computes correctly
- Output not corrupted by subsequent state update
- No pointer aliasing between output and state regions (verified at runtime)

### 3. Kernel memory layout has no overlaps between threads
- Thread partitioning: `heads_per_thread = ceil(32/12) = 3`, contiguous head ranges
- Output: each head writes to `out_data[h*128 + t*4096 ... h*128+127 + t*4096]` — non-overlapping
- State: each head owns `state_out[h*16384 ... (h+1)*16384-1]` — non-overlapping
- `v_new_buf`: per-thread malloc — thread-local
- `state_in` and `dst`: NOT aliased (confirmed by allocator code AND runtime pointer check)

### 4. GGML barriers are solid
- `ggml_barrier()` after every op in graph compute thread loop (line 26266)
- OpenMP `#pragma omp barrier` when using OpenMP (line 4504)
- All threads sync before proceeding to next graph node

### 5. The race is OUTSIDE the kernel
- **v14 proof:** Forced `if (params->ith != 0) return;` in the kernel, so only thread 0 runs it (all 32 heads, single-threaded). With `-t 12`, output is still garbage.
- **v14 with `-t 1`:** Works perfectly.
- Therefore, the multi-threading bug is in OTHER ops that run with 12 threads.

## Architecture: Three DeltaNet Paths

### Fused Path (default)
```
Input [S,H,T,B] → permute+cont → [S,T,H,B] → ggml_delta_net kernel → view+cont → output [S,H,T,B]
```
Uses `ggml_cont_4d(ggml_permute(...))` for q, k, v, g, beta (5 permute+cont ops).

### Autoregressive Path (`--no-fused-delta`, T=1)
```
Input [S,H,1,B] → standard GGML ops (mul, transpose, sum_rows, etc.) → output
```
No permute+cont pattern. All well-tested ops.

### Chunked Path (`--no-fused-delta`, T>1)
```
Input → chunk processing via GGML ops → output
```
Also no custom kernel.

## Key Hypothesis: `ggml_cont_4d(ggml_permute(...))` Threading Bug (T=1 specific)

The ONLY difference between the working non-fused path and the broken fused path:
- Fused: 5x `ggml_cont_4d(ggml_permute(...))` ops + 1x ggml_delta_net kernel + 1x output cont
- Non-fused: Standard GGML ops (mul_mat, transpose, sum_rows, etc.)

The `GGML_OP_CONT` op handles copying from non-contiguous (permuted) source to contiguous destination. With 12 threads, each thread copies a portion. If this multi-threaded copy has a bug for specific permutation patterns, it would corrupt the kernel's inputs.

**Refined by ikawrakow's PPL data:** The fused path produces **correct PPL during prefill** (T>1) but **garbage during generation** (T=1). This means the CONT threading bug is **shape-dependent** — it triggers when T=1 (small inner dimension after permute) but not when T is large. With T=1, the work partitioning across 12 threads may create degenerate ranges or off-by-one errors in `ggml_compute_forward_dup_f32`.

**Why this wasn't caught before:** Standard transformer attention uses permute(0,2,1,3) too, but with T=1 the tensor is already effectively contiguous in the permuted dimensions, so the CONT op is a no-op or trivial copy. The delta-net permute pattern applied to 4D tensors with T=1 may exercise a rarely-tested code path.

## Proposed Fix: Hybrid Dispatch

The dispatch at `src/llama-build-context.cpp:~4980` currently sends ALL tokens through fused when enabled:

```cpp
// CURRENT (broken for T=1 generation)
if (use_fused_delta_net) {
    attn_out = build_delta_net_fused(q, k, v, gate, beta, state, il);
} else {
    attn_out = n_tok == 1
        ? build_delta_net_autoregressive(...)
        : build_delta_net_chunking(...);
}
```

Proposed fix — fused for prefill only, autoregressive for generation:

```cpp
// FIXED: fused for T>1 (correct PPL), autoregressive for T=1 (correct output)
if (use_fused_delta_net && n_tok > 1) {
    attn_out = build_delta_net_fused(q, k, v, gate, beta, state, il);
} else if (n_tok == 1) {
    attn_out = build_delta_net_autoregressive(q, k, v, gate, beta, state, il);
} else {
    attn_out = build_delta_net_chunking(q, k, v, gate, beta, state, causal_mask, identity, diag_mask, il);
}
```

This sidesteps the CONT threading bug for generation and the chunked PPL degradation for prefill. Expected performance: ~110+ t/s PP (fused) + ~7-11 t/s TG (autoregressive) with correct output.

## Required Patches (Working Non-Fused Build)

### Patch 1: SSM_DT tensor name compat (Ollama GGUF)
```bash
sed -i 's|tn(LLM_TENSOR_SSM_DT,         "bias",   i), {hparams.ssm_dt_rank}|tn(LLM_TENSOR_SSM_DT,                    i), {hparams.ssm_dt_rank}|' src/llama-load-tensors.cpp
```

### Patch 2: Per-layer n_embd_k/v_gqa
```bash
sed -i '/\/\/ Full-attention layer/a\            const int64_t n_embd_k_gqa = hparams.n_embd_k_gqa(i);\n            const int64_t n_embd_v_gqa = hparams.n_embd_v_gqa(i);' src/llama-load-tensors.cpp
```

### Patch 3: Per-layer n_head_kv
```bash
sed -i '/build_layer_attn = \[&\]/a\        const int64_t n_head_kv = hparams.n_head_kv(il);' src/llama-build-context.cpp
```

### Patch 4: g permutation (verified no-op, kept for consistency)
```bash
sed -i 's|ggml_permute(ctx0, g, 2, 0, 3, 1)|ggml_permute(ctx0, g, 1, 0, 2, 3)|' src/llama-build-context.cpp
```

## Performance Baseline

### Our hardware (AMD Ryzen AI 9 HX PRO 370, 12c/24t, CPU-only, Q4_K)

| Config | Prompt tok/s | Gen tok/s |
|--------|-------------|-----------|
| Ollama (baseline) | — | 7.74 |
| `--no-fused-delta -t 12` | ~63 | ~9.5 |
| `-t 1` fused | ~10 | ~15.6 |

### ikawrakow's hardware (Ryzen 3995WX, CPU-only, IQ4_XS)

| Config | PP-2048 tok/s | TG-128 tok/s | PPL [1] |
|--------|--------------|-------------|---------|
| Upstream llama.cpp | 70.4 | 7.23 | 2.99 |
| PR #1251, fused on, FA on | 110.6 | 11.63 | 3.01 (correct) |
| PR #1251, fused off, FA on | 162 | 7.25 | 4.17 (degraded) |
| PR #1251, fused on, FA off | — | — | 17.5 (broken) |

**Key insight:** Fused kernel produces correct PPL during prefill (T>1) but garbage during generation (T=1). Non-fused PP is actually faster (162 vs 110 t/s) but has degraded PPL from the chunked path bug. Non-fused TG matches upstream (7.25 vs 7.23).

## Docker Images on NAS

| Image | Description | Status |
|-------|-------------|--------|
| `ik-pr1251-v7` | Patches 1-4, no kernel mods | Working with `--no-fused-delta` |
| `ik-pr1251-v12-deep` | Deep debug instrumentation | Proved kernel math correct |
| `ik-pr1251-v13-ntasks1` | n_tasks=1 attempt | Failed (nth still 12 in kernel) |
| `ik-pr1251-v14` | Single-thread kernel patch | Proved race is outside kernel |

## Next Steps

### Priority 1: Test hybrid dispatch fix
- Patch the dispatch to use fused for T>1, autoregressive for T=1
- Verify correct output AND correct PPL with multi-threading
- This is likely a one-session task

### Priority 2: Root-cause the CONT threading bug (T=1)
- Read `ggml_compute_forward_dup_f32` — find the f32-from-permuted code path
- Determine why T=1 tensor shapes trigger the race but T>1 doesn't
- Test with `-t 2` to find minimum thread count for failure

### Priority 3: Root-cause chunked path PPL degradation
- Compare `build_delta_net_chunking` against upstream's chunked implementation
- Codex may have "invented" a different chunking algorithm (same pattern as the fused kernel)

### Priority 4: Address Codex regressions
- ikawrakow noted Codex reverted his MoE row counting optimization (the major MoE bottleneck fix)
- CUDA PP is 3.3x slower than upstream because of these regressions
- Codex also disabled FA by default for CPU — need to remove that override

### PR Comment — DONE
- Posted findings: https://github.com/ikawrakow/ik_llama.cpp/pull/1251#issuecomment-3879689776
- ikawrakow independently confirmed CPU broken, suggested copying mainline implementation

## Agent Architecture Recommendation

For the next debugging session, use a **two-agent collaborative approach** — pair debugging, not assembly line:

### Agent A: "Analyst" (Opus)
- Reads codebase, forms hypotheses, spots patterns in results
- Proposes next investigative step (read specific code, try specific patch, check specific value)

### Agent B: "Operator" (Bash-capable)
- Reads source, runs builds, executes tests, reports raw results
- But also **pushes back** — "that hypothesis doesn't hold because I see X at line Y"

### Key: Conversational, not handoff-based

The agents discuss at each stage rather than operating in strict plan→execute cycles. Agent B isn't a dumb executor — it reads code too and challenges Agent A's assumptions. Agent A doesn't wait for a full build-test cycle to pivot — if Agent B finds contradicting evidence while reading source, they course-correct immediately.

```
A: "What if CONT miscalculates row stride for permute(0,2,1,3) when ne[1] != ne[2]?"
B: "Reading ggml_compute_forward_dup_f32... no, the stride calc looks correct for that case.
    But look — nb[2] here assumes contiguous src, and the permuted tensor isn't contiguous."
A: "That's it. The nb values after permute are reordered but dup_f32 indexes with original
    dimension order. Check if this specific permute pattern [0,2,1,3] hits a different code
    branch than standard attention permutes."
B: [reads code, confirms] "Yes — standard attention uses permute(0,2,1,3) too but the tensor
    shapes are different. For delta_net, ne[1]=T=128 and ne[2]=H=32, so the work partitioning
    across 12 threads splits differently."
```

This prevents:
1. Single-agent context exhaustion (analysis + execution compete for window space)
2. Rigid plan→test loops that waste build cycles on already-invalidated hypotheses
3. One agent missing something the other would catch in real-time

## Key Source Locations

| File | Line | What |
|------|------|------|
| `ggml/src/ggml.c` | 21939 | `ggml_compute_forward_delta_net_f32` — the kernel |
| `ggml/src/ggml.c` | 10235 | `ggml_delta_net()` — op creation, tensor shapes |
| `ggml/src/ggml.c` | 25957 | n_tasks = n_threads for DELTA_NET |
| `ggml/src/ggml.c` | 26239 | Graph compute thread loop with barrier |
| `ggml/src/ggml.c` | 4499 | `ggml_barrier()` implementation |
| `ggml/src/ggml-alloc.c` | 40 | `ggml_op_can_inplace()` — DELTA_NET not listed |
| `src/llama-build-context.cpp` | 4616 | `build_delta_net_fused` — permute+cont ops |
| `src/llama-build-context.cpp` | 4551 | `build_delta_net_autoregressive` — working path |
| `src/llama-build-context.cpp` | 4343 | `build_delta_net_chunking` — prompt path |
| `src/llama-build-context.cpp` | 4980 | Dispatch: fused vs autoregressive vs chunked |
